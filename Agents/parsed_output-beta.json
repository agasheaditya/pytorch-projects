[
  {
    "code": "# Installing the libraries with the specified version.\n#!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.5.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user\n\n# to load and manipulate data\nimport pandas as pd\nimport numpy as np\n\n# to visualize data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to split data into training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# to build decision tree model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# to tune different models\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n\nfrom xgboost import XGBClassifier\n\n# To undersample and oversample the data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport sklearn.metrics as metrics\n\n# to compute classification metrics\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    recall_score,\n    precision_score,\n    f1_score,\n)",
    "output": "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).",
    "image_path": null
  },
  {
    "code": "from google.colab import drive\ndrive.mount('/content/drive')",
    "output": "   Unnamed: 0 case_id continent education_of_employee has_job_experience requires_job_training  no_of_employees  yr_of_estab region_of_employment  prevailing_wage unit_of_wage full_time_position case_status\n0           0  EZYV01      Asia           High School                  N                     N            14513         2007                 West         592.2029         Hour                  Y      Denied\n1           1  EZYV02      Asia              Master's                  Y                     N             2412         2002            Northeast       83425.6500         Year                  Y   Certified\n2           2  EZYV03      Asia            Bachelor's                  N                     Y            44444         2008                 West      122996.8600         Year                  Y      Denied\n3           3  EZYV04      Asia            Bachelor's                  N                     N               98         1897                 West       83434.0300         Year                  Y      Denied\n4           4  EZYV05    Africa              Master's                  Y                     N             1082         2005                South      149907.3900         Year                  Y   Certified",
    "image_path": null
  },
  {
    "code": "visa_data = pd.read_csv(\"/content/drive/MyDrive/Personal/Great Learning/Advanced Machine Learning Course/Assignment/EasyVisa.csv\")\n\ndata = visa_data.copy()",
    "output": "   Unnamed: 0    case_id continent education_of_employee has_job_experience requires_job_training  no_of_employees  yr_of_estab region_of_employment  prevailing_wage unit_of_wage full_time_position case_status\n0       25475  EZYV25476      Asia            Bachelor's                  Y                     Y             2601         2008                South         77092.57         Year                  Y   Certified\n1       25476  EZYV25477      Asia           High School                  Y                     N             3274         2006            Northeast        279174.79         Year                  Y   Certified\n2       25477  EZYV25478      Asia              Master's                  Y                     N             1121         1910                South        146298.85         Year                  N   Certified\n3       25478  EZYV25479      Asia              Master's                  Y                     Y             1918         1887                 West         86154.77         Year                  Y   Certified\n4       25479  EZYV25480      Asia            Bachelor's                  Y                     N             3195         1960              Midwest         70876.91         Year                  Y   Certified",
    "image_path": null
  },
  {
    "code": "data.head(5)",
    "output": "(25480, 12)",
    "image_path": null
  },
  {
    "code": "data.tail(5)",
    "output": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25480 entries, 0 to 25479\nData columns (total 12 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   case_id                25480 non-null  object \n 1   continent              25480 non-null  object \n 2   education_of_employee  25480 non-null  object \n 3   has_job_experience     25480 non-null  object \n 4   requires_job_training  25480 non-null  object \n 5   no_of_employees        25480 non-null  int64  \n 6   yr_of_estab            25480 non-null  int64  \n 7   region_of_employment   25480 non-null  object \n 8   prevailing_wage        25480 non-null  float64\n 9   unit_of_wage           25480 non-null  object \n 10  full_time_position     25480 non-null  object \n 11  case_status            25480 non-null  object \ndtypes: float64(1), int64(2), object(9)\nmemory usage: 2.3+ MB",
    "image_path": null
  },
  {
    "code": "data.shape",
    "output": "              Unnamed: 0    count   unique         top     freq  mean  std  min  25%  50%  75%  max\n0                case_id  25480.0  25480.0   EZYV25480      1.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN\n1              continent  25480.0      6.0        Asia  16861.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN\n2  education_of_employee  25480.0      4.0  Bachelor's  10234.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3     has_job_experience  25480.0      2.0           Y  14802.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN\n4  requires_job_training  25480.0      2.0           N  22525.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN",
    "image_path": null
  },
  {
    "code": "data.info()",
    "output": "Number of negative entries: 33",
    "image_path": null
  },
  {
    "code": "data.describe(include=\"all\").T",
    "output": "Number of negative entries: 0",
    "image_path": null
  },
  {
    "code": "negative_count = (data['no_of_employees'] < 0).sum()\nprint(f\"Number of negative entries: {negative_count}\")",
    "output": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25480 entries, 0 to 25479\nData columns (total 12 columns):\n #   Column                 Non-Null Count  Dtype   \n---  ------                 --------------  -----   \n 0   case_id                25480 non-null  object  \n 1   continent              25480 non-null  category\n 2   education_of_employee  25480 non-null  category\n 3   has_job_experience     25480 non-null  category\n 4   requires_job_training  25480 non-null  category\n 5   no_of_employees        25480 non-null  float64 \n 6   yr_of_estab            25480 non-null  int64   \n 7   region_of_employment   25480 non-null  category\n 8   prevailing_wage        25480 non-null  float64 \n 9   unit_of_wage           25480 non-null  category\n 10  full_time_position     25480 non-null  category\n 11  case_status            25480 non-null  category\ndtypes: category(8), float64(2), int64(1), object(1)\nmemory usage: 996.7+ KB",
    "image_path": null
  },
  {
    "code": "mean_value = data.loc[data['no_of_employees'] >= 0, 'no_of_employees'].mean()\ndata['no_of_employees'] = data['no_of_employees'].apply(lambda x: x if x >= 0 else mean_value)",
    "output": "Value counts for column: continent\ncontinent\nAsia             16861\nEurope            3732\nNorth America     3292\nSouth America      852\nAfrica             551\nOceania            192\nName: count, dtype: int64\n\nValue counts for column: education_of_employee\neducation_of_employee\nBachelor's     10234\nMaster's        9634\nHigh School     3420\nDoctorate       2192\nName: count, dtype: int64\n\nValue counts for column: has_job_experience\nhas_job_experience\nY    14802\nN    10678\nName: count, dtype: int64\n\nValue counts for column: requires_job_training\nrequires_job_training\nN    22525\nY     2955\nName: count, dtype: int64\n\nValue counts for column: region_of_employment\nregion_of_employment\nNortheast    7195\nSouth        7017\nWest         6586\nMidwest      4307\nIsland        375\nName: count, dtype: int64\n\nValue counts for column: unit_of_wage\nunit_of_wage\nYear     22962\nHour      2157\nWeek       272\nMonth       89\nName: count, dtype: int64\n\nValue counts for column: full_time_position\nfull_time_position\nY    22773\nN     2707\nName: count, dtype: int64\n\nValue counts for column: case_status\ncase_status\nCertified    17018\nDenied        8462\nName: count, dtype: int64",
    "image_path": null
  },
  {
    "code": "negative_count = (data['no_of_employees'] < 0).sum()\nprint(f\"Number of negative entries: {negative_count}\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_1.png"
  },
  {
    "code": "data['continent'] = data['continent'].astype('category')\ndata['education_of_employee'] = data['education_of_employee'].astype('category')\ndata['has_job_experience'] = data['has_job_experience'].astype('category')\ndata['requires_job_training'] = data['requires_job_training'].astype('category')\ndata['region_of_employment'] = data['region_of_employment'].astype('category')\ndata['unit_of_wage'] = data['unit_of_wage'].astype('category')\ndata['full_time_position'] = data['full_time_position'].astype('category')\ndata['case_status'] = data['case_status'].astype('category')",
    "output": "<ipython-input-202-1e58b03272cc>:22: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(",
    "image_path": null
  },
  {
    "code": "data.info()",
    "output": "{\"Bachelor's\": np.int64(0), 'Doctorate': np.int64(1), 'High School': np.int64(2), \"Master's\": np.int64(3)}",
    "image_path": null
  },
  {
    "code": "categorical_cols = data.select_dtypes(include=['category']).columns\n\nfor col in categorical_cols:\n    print(f\"\\nValue counts for column: {col}\")\n    print(data[col].value_counts())",
    "output": "",
    "image_path": "extracted_images_beta\\image_2.png"
  },
  {
    "code": "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (15,10))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a triangle will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram",
    "output": "<ipython-input-202-1e58b03272cc>:22: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(",
    "image_path": null
  },
  {
    "code": "# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot",
    "output": "{'Island': np.int64(0), 'Midwest': np.int64(1), 'Northeast': np.int64(2), 'South': np.int64(3), 'West': np.int64(4)}",
    "image_path": null
  },
  {
    "code": "#data['education_of_employee'] = data['education_of_employee'].astype('category')\nle = LabelEncoder()\n\n#converting below to numbers so that above function histogram_boxplot can be used to call mean without error\ndata['education_of_employee'] = le.fit_transform(data['education_of_employee'])\n\nhistogram_boxplot(data, \"education_of_employee\", figsize=(15, 10))",
    "output": "",
    "image_path": "extracted_images_beta\\image_3.png"
  },
  {
    "code": "labeled_barplot(data, 'education_of_employee')",
    "output": "<ipython-input-202-1e58b03272cc>:22: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(",
    "image_path": null
  },
  {
    "code": "mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(mapping)",
    "output": "{'N': np.int64(0), 'Y': np.int64(1)}",
    "image_path": null
  },
  {
    "code": "#converting below to numbers so that above function histogram_boxplot can be used to call mean without error\ndata['region_of_employment'] = le.fit_transform(data['region_of_employment'])\n\nhistogram_boxplot(data, \"region_of_employment\", figsize=(15, 10))",
    "output": "",
    "image_path": "extracted_images_beta\\image_4.png"
  },
  {
    "code": "labeled_barplot(data, 'region_of_employment')",
    "output": "<ipython-input-202-1e58b03272cc>:22: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.countplot(",
    "image_path": null
  },
  {
    "code": "mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(mapping)",
    "output": "{'Certified': np.int64(0), 'Denied': np.int64(1)}",
    "image_path": null
  },
  {
    "code": "#converting below to numbers so that above function histogram_boxplot can be used to call mean without error\ndata['has_job_experience'] = le.fit_transform(data['has_job_experience'])\n\nhistogram_boxplot(data, \"has_job_experience\", figsize=(15, 10))",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "labeled_barplot(data, 'has_job_experience')",
    "output": "case_status                0     1    All\neducation_of_employee                    \nAll                    17018  8462  25480\n0                       6367  3867  10234\n2                       1164  2256   3420\n3                       7575  2059   9634\n1                       1912   280   2192\n------------------------------------------------------------------------------------------------------------------------",
    "image_path": null
  },
  {
    "code": "mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(mapping)",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "data['case_status'] = le.fit_transform(data['case_status'])\n\nhistogram_boxplot(data, \"case_status\", figsize=(15, 10))",
    "output": "case_status        0     1    All\ncontinent                        \nAll            17018  8462  25480\nAsia           11012  5849  16861\nNorth America   2037  1255   3292\nEurope          2957   775   3732\nSouth America    493   359    852\nAfrica           397   154    551\nOceania          122    70    192\n------------------------------------------------------------------------------------------------------------------------",
    "image_path": null
  },
  {
    "code": "labeled_barplot(data, 'case_status')",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(mapping)",
    "output": "case_status             0     1    All\nhas_job_experience                    \nAll                 17018  8462  25480\n0                    5994  4684  10678\n1                   11024  3778  14802\n------------------------------------------------------------------------------------------------------------------------",
    "image_path": null
  },
  {
    "code": "### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "def stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "distribution_plot_wrt_target(data, \"education_of_employee\", \"case_status\")",
    "output": "<ipython-input-215-d405489ef7b9>:31: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n<ipython-input-215-d405489ef7b9>:34: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(",
    "image_path": null
  },
  {
    "code": "stacked_barplot(data, \"education_of_employee\", \"case_status\")",
    "output": "case_status       0     1    All\nunit_of_wage                    \nAll           17018  8462  25480\nYear          16047  6915  22962\nHour            747  1410   2157\nWeek            169   103    272\nMonth            55    34     89\n------------------------------------------------------------------------------------------------------------------------",
    "image_path": null
  },
  {
    "code": "distribution_plot_wrt_target(data, \"continent\", \"case_status\")",
    "output": "              Unnamed: 0  0\n0                case_id  0\n1              continent  0\n2  education_of_employee  0\n3     has_job_experience  0\n4  requires_job_training  0",
    "image_path": null
  },
  {
    "code": "stacked_barplot(data, \"continent\", \"case_status\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_5.png"
  },
  {
    "code": "distribution_plot_wrt_target(data, \"has_job_experience\", \"case_status\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_6.png"
  },
  {
    "code": "stacked_barplot(data, \"has_job_experience\", \"case_status\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_7.png"
  },
  {
    "code": "distribution_plot_wrt_target(data, \"prevailing_wage\", \"region_of_employment\")",
    "output": "  Unnamed: 0_level_0         proportion\n         case_status Unnamed: 1_level_1\n0                  0           0.667896\n1                  1           0.332104",
    "image_path": null
  },
  {
    "code": "distribution_plot_wrt_target(data, \"prevailing_wage\", \"case_status\")",
    "output": "   Unnamed: 0  education_of_employee  has_job_experience  no_of_employees  yr_of_estab  region_of_employment  prevailing_wage  continent_Asia  continent_Europe  continent_North America  continent_Oceania  continent_South America  requires_job_training_Y  unit_of_wage_Month  unit_of_wage_Week  unit_of_wage_Year  full_time_position_Y\n0           0                    2.0                 0.0          14513.0       2007.0                   4.0         592.2029             1.0               0.0                      0.0                0.0                      0.0                      0.0                 0.0                0.0                0.0                   1.0\n1           1                    3.0                 1.0           2412.0       2002.0                   2.0       83425.6500             1.0               0.0                      0.0                0.0                      0.0                      0.0                 0.0                0.0                1.0                   1.0\n2           2                    0.0                 0.0          44444.0       2008.0                   4.0      122996.8600             1.0               0.0                      0.0                0.0                      0.0                      1.0                 0.0                0.0                1.0                   1.0\n3           3                    0.0                 0.0             98.0       1897.0                   4.0       83434.0300             1.0               0.0                      0.0                0.0                      0.0                      0.0                 0.0                0.0                1.0                   1.0\n4           4                    3.0                 1.0           1082.0       2005.0                   3.0      149907.3900             0.0               0.0                      0.0                0.0                      0.0                      0.0                 0.0                0.0                1.0                   1.0",
    "image_path": null
  },
  {
    "code": "distribution_plot_wrt_target(data, \"unit_of_wage\", \"case_status\")",
    "output": "   Unnamed: 0  case_status\n0           0            1\n1           1            0\n2           2            1\n3           3            1\n4           4            0",
    "image_path": null
  },
  {
    "code": "stacked_barplot(data, \"unit_of_wage\", \"case_status\")",
    "output": "Shape of training set: (20384, 16)\nShape of test set: (5096, 16) \n\nPercentage of classes in training set:\ncase_status\n0    66.787677\n1    33.212323\nName: proportion, dtype: float64 \n\nPercentage of classes in test set:\ncase_status\n0    66.797488\n1    33.202512\nName: proportion, dtype: float64",
    "image_path": null
  },
  {
    "code": "# checking missing values in the data\ndata.isna().sum()",
    "output": "DecisionTreeClassifier(random_state=42)",
    "image_path": null
  },
  {
    "code": "# Case status vs Education of employee\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='case_status', y='education_of_employee');",
    "output": "",
    "image_path": "extracted_images_beta\\image_8.png"
  },
  {
    "code": "# Case status vs has_job_experience\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='case_status', y='has_job_experience');",
    "output": "   Unnamed: 0  Accuracy  Recall  Precision   F1\n0           0       1.0     1.0        1.0  1.0",
    "image_path": null
  },
  {
    "code": "# \"unit_of_wage\", \"case_status\"\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=data, x='unit_of_wage', y='case_status');",
    "output": "",
    "image_path": "extracted_images_beta\\image_9.png"
  },
  {
    "code": "# checking the distribution of the target variable\ndata[\"case_status\"].value_counts(1)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.648744  0.470449   0.471006  0.470727",
    "image_path": null
  },
  {
    "code": "# defining the explanatory (independent) and response (dependent) variables\nX = data.drop([\"case_status\", \"case_id\"], axis=1)\ny = data[\"case_status\"]",
    "output": "",
    "image_path": "extracted_images_beta\\image_10.png"
  },
  {
    "code": "# creating dummy variables\nX = pd.get_dummies(X, columns=X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(), drop_first=True)\n\n# specifying the datatype of the independent variables data frame\nX = X.astype(float)\n\nX.head()",
    "output": "DecisionTreeClassifier(max_depth=np.int64(8), max_leaf_nodes=np.int64(10),\n                       min_samples_split=np.int64(10), random_state=42)",
    "image_path": null
  },
  {
    "code": "#no need to change y as it is already 0 or 1\ny.head()",
    "output": "",
    "image_path": "extracted_images_beta\\image_11.png"
  },
  {
    "code": "# splitting the data in an 80:20 ratio for train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)    # stratify ensures that the training and test sets have a similar distribution of the response variable",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.732094  0.566765   0.602828  0.584241",
    "image_path": null
  },
  {
    "code": "print(\"Shape of training set:\", X_train.shape)\nprint(\"Shape of test set:\", X_test.shape, '\\n')\nprint(\"Percentage of classes in training set:\")\nprint(100*y_train.value_counts(normalize=True), '\\n')\nprint(\"Percentage of classes in test set:\")\nprint(100*y_test.value_counts(normalize=True))",
    "output": "",
    "image_path": "extracted_images_beta\\image_12.png"
  },
  {
    "code": "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n\n\ndef model_performance_classification_sklearn(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred)  # to compute Recall\n    precision = precision_score(target, pred)  # to compute Precision\n    f1 = f1_score(target, pred)  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n        index=[0],\n    )\n\n    return df_perf",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.731358  0.560875    0.60254  0.580961",
    "image_path": null
  },
  {
    "code": "def confusion_matrix_sklearn(model, predictors, target):\n    \"\"\"\n    To plot the confusion_matrix with percentages\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n    y_pred = model.predict(predictors)\n    cm = confusion_matrix(target, y_pred)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(2, 2)\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")",
    "output": "BaggingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# creating an instance of the decision tree model\ndtree1 = DecisionTreeClassifier(random_state=42)    # random_state sets a seed value and enables reproducibility\n\n# fitting the model to the training data\ndtree1.fit(X_train, y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_13.png"
  },
  {
    "code": "confusion_matrix_sklearn(dtree1, X_train, y_train)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.980033  0.945495   0.994098  0.969188",
    "image_path": null
  },
  {
    "code": "dtree1_train_perf = model_performance_classification_sklearn(\n    dtree1, X_train, y_train\n)\ndtree1_train_perf",
    "output": "",
    "image_path": "extracted_images_beta\\image_14.png"
  },
  {
    "code": "confusion_matrix_sklearn(dtree1, X_test, y_test)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.703297  0.422577      0.572  0.486064",
    "image_path": null
  },
  {
    "code": "dtree1_test_perf = model_performance_classification_sklearn(\n    dtree1, X_test, y_test\n)\ndtree1_test_perf",
    "output": "RandomForestClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# list of feature names in X_train\nfeature_names = list(X_train.columns)\n\n# set the figure size for the plot\nplt.figure(figsize=(20, 20))\n\n# plotting the decision tree\nout = tree.plot_tree(\n    dtree1,                         # decision tree classifier model\n    feature_names=feature_names,    # list of feature names (columns) in the dataset\n    filled=True,                    # fill the nodes with colors based on class\n    fontsize=9,                     # font size for the node text\n    node_ids=False,                 # do not show the ID of each node\n    class_names=None,               # whether or not to display class names\n)\n\n# add arrows to the decision tree splits if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")    # set arrow color to black\n        arrow.set_linewidth(1)          # set arrow linewidth to 1\n\n# displaying the plot\nplt.show()",
    "output": "",
    "image_path": "extracted_images_beta\\image_15.png"
  },
  {
    "code": "# define the parameters of the tree to iterate over\nmax_depth_values = np.arange(2, 11, 2)\nmax_leaf_nodes_values = np.arange(10, 51, 10)\nmin_samples_split_values = np.arange(10, 51, 10)\n\n# initialize variables to store the best model and its performance\nbest_estimator = None\nbest_score_diff = float('inf')\n\n# iterate over all combinations of the specified parameter values\nfor max_depth in max_depth_values:\n    for max_leaf_nodes in max_leaf_nodes_values:\n        for min_samples_split in min_samples_split_values:\n\n            # initialize the tree with the current set of parameters\n            estimator = DecisionTreeClassifier(\n                max_depth=max_depth,\n                max_leaf_nodes=max_leaf_nodes,\n                min_samples_split=min_samples_split,\n                random_state=42\n            )\n\n            # fit the model to the training data\n            estimator.fit(X_train, y_train)\n\n            # make predictions on the training and test sets\n            y_train_pred = estimator.predict(X_train)\n            y_test_pred = estimator.predict(X_test)\n\n            # calculate F1 scores for training and test sets\n            train_f1_score = f1_score(y_train, y_train_pred)\n            test_f1_score = f1_score(y_test, y_test_pred)\n\n            # calculate the absolute difference between training and test F1 scores\n            score_diff = abs(train_f1_score - test_f1_score)\n\n            # update the best estimator and best score if the current one has a smaller score difference\n            if score_diff < best_score_diff:\n                best_score_diff = score_diff\n                best_estimator = estimator",
    "output": "   Unnamed: 0  Accuracy  Recall  Precision   F1\n0           0       1.0     1.0        1.0  1.0",
    "image_path": null
  },
  {
    "code": "# creating an instance of the best model\ndtree2 = best_estimator\n\n# fitting the best model to the training data\ndtree2.fit(X_train, y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_16.png"
  },
  {
    "code": "confusion_matrix_sklearn(dtree2, X_train, y_train)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.720958  0.469858   0.602273  0.527888",
    "image_path": null
  },
  {
    "code": "dtree2_train_perf = model_performance_classification_sklearn(\n    dtree2, X_train, y_train\n)\ndtree2_train_perf",
    "output": "AdaBoostClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "confusion_matrix_sklearn(dtree2, X_test, y_test)",
    "output": "",
    "image_path": "extracted_images_beta\\image_17.png"
  },
  {
    "code": "dtree2_test_perf = model_performance_classification_sklearn(\n    dtree2, X_test, y_test\n)\ndtree2_test_perf",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision       F1\n0           0  0.733713  0.387888   0.671611  0.49176",
    "image_path": null
  },
  {
    "code": "#base_estimator for bagging classifier is a decision tree by default\nbagging_estimator=BaggingClassifier(random_state=1)\nbagging_estimator.fit(X_train,y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_18.png"
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(bagging_estimator, X_train, y_train)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.728218  0.381206   0.656155  0.482243",
    "image_path": null
  },
  {
    "code": "bagging_estimator_train_perf = model_performance_classification_sklearn(\n    bagging_estimator, X_train, y_train\n)\nbagging_estimator_train_perf",
    "output": "GradientBoostingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(bagging_estimator, X_test, y_test)",
    "output": "",
    "image_path": "extracted_images_beta\\image_19.png"
  },
  {
    "code": "bagging_estimator_test_perf = model_performance_classification_sklearn(\n    bagging_estimator, X_test, y_test\n)\nbagging_estimator_test_perf",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.754857  0.512555    0.67157  0.581386",
    "image_path": null
  },
  {
    "code": "#Train the random forest classifier\nrf_estimator=RandomForestClassifier(random_state=1)\nrf_estimator.fit(X_train,y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_20.png"
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(rf_estimator, X_train, y_train)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision       F1\n0           0  0.742739  0.497636   0.646201  0.56227",
    "image_path": null
  },
  {
    "code": "rf_estimator_train_perf = model_performance_classification_sklearn(\n    rf_estimator, X_train, y_train\n)\nrf_estimator_train_perf",
    "output": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=1, ...)",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(rf_estimator, X_test, y_test)",
    "output": "",
    "image_path": "extracted_images_beta\\image_21.png"
  },
  {
    "code": "rf_estimator_test_perf = model_performance_classification_sklearn(\n    rf_estimator, X_test, y_test\n)\nrf_estimator_test_perf",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.839973  0.660709   0.822545  0.732798",
    "image_path": null
  },
  {
    "code": "adaboost = AdaBoostClassifier(random_state=1)\nadaboost.fit(X_train,y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_22.png"
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(adaboost, X_train, y_train)",
    "output": "   Unnamed: 0  Accuracy    Recall  Precision        F1\n0           0  0.724294  0.462175   0.612373  0.526777",
    "image_path": null
  },
  {
    "code": "adaboost_train_perf = model_performance_classification_sklearn(\n    adaboost, X_train, y_train\n)\nadaboost_train_perf",
    "output": "(12740, 16) (7644, 16) (5096, 16)",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(adaboost, X_test, y_test)",
    "output": "Target value ratio in y\ncase_status\n0    0.667896\n1    0.332104\nName: proportion, dtype: float64\n********************************************************************************\nTarget value ratio in y_train\ncase_status\n0    0.667896\n1    0.332104\nName: proportion, dtype: float64\n********************************************************************************\nTarget value ratio in y_val\ncase_status\n0    0.667844\n1    0.332156\nName: proportion, dtype: float64\n********************************************************************************\nTarget value ratio in y_test\ncase_status\n0    0.667975\n1    0.332025\nName: proportion, dtype: float64\n********************************************************************************",
    "image_path": null
  },
  {
    "code": "adaboost_test_perf = model_performance_classification_sklearn(\n    adaboost, X_test, y_test\n)\nadaboost_test_perf",
    "output": "Before OverSampling, count of label '1': 4231\nBefore OverSampling, count of label '0': 8509 \n\nAfter OverSampling, count of label '1': 8509\nAfter OverSampling, count of label '0': 8509 \n\nAfter OverSampling, the shape of train_X: (17018, 16)\nAfter OverSampling, the shape of train_y: (17018,)",
    "image_path": null
  },
  {
    "code": "gboost = GradientBoostingClassifier(random_state=1)\ngboost.fit(X_train,y_train)",
    "output": "DecisionTreeClassifier(max_depth=np.int64(8), max_leaf_nodes=np.int64(10),\n                       min_samples_split=np.int64(10), random_state=42)",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(gboost, X_train, y_train)",
    "output": "0.7313432835820896\n0.551792044111855",
    "image_path": null
  },
  {
    "code": "gboost_train_perf = model_performance_classification_sklearn(\n    gboost, X_train, y_train\n)\ngboost_train_perf",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(gboost, X_test, y_test)",
    "output": "",
    "image_path": "extracted_images_beta\\image_23.png"
  },
  {
    "code": "gboost_test_perf = model_performance_classification_sklearn(\n    gboost, X_test, y_test\n)\ngboost_test_perf",
    "output": "BaggingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "xgboost = XGBClassifier(random_state=1,eval_metric='logloss')\nxgboost.fit(X_train, y_train)",
    "output": "0.9734398871782818\n0.435998424576605",
    "image_path": null
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(xgboost, X_train, y_train)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "xgboost_train_perf = model_performance_classification_sklearn(\n    xgboost, X_train, y_train\n)\nxgboost_train_perf",
    "output": "",
    "image_path": "extracted_images_beta\\image_24.png"
  },
  {
    "code": "#Using above defined function to get accuracy, recall and precision on train and test set\nconfusion_matrix_sklearn(xgboost, X_test, y_test)",
    "output": "RandomForestClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "xgboost_test_perf = model_performance_classification_sklearn(\n    xgboost, X_test, y_test\n)\nxgboost_test_perf",
    "output": "0.9998824773768951\n0.49192595510043324",
    "image_path": null
  },
  {
    "code": "# Splitting data into training, validation and test set:\n\n# first we split data into 2 parts, say temporary and test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.5, random_state=0, stratify=y\n)\n\n# then we split the temporary set into train and validation\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.4, random_state=0, stratify=y_temp\n)\n\nprint(X_train.shape, X_val.shape, X_test.shape)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Checking class balance for whole data, train set, validation set, and test set\n\nprint(\"Target value ratio in y\")\nprint(y.value_counts(1))\nprint(\"*\" * 80)\nprint(\"Target value ratio in y_train\")\nprint(y_train.value_counts(1))\nprint(\"*\" * 80)\nprint(\"Target value ratio in y_val\")\nprint(y_val.value_counts(1))\nprint(\"*\" * 80)\nprint(\"Target value ratio in y_test\")\nprint(y_test.value_counts(1))\nprint(\"*\" * 80)",
    "output": "",
    "image_path": "extracted_images_beta\\image_25.png"
  },
  {
    "code": "# Fit SMOTE on train data(Synthetic Minority Oversampling Technique)\nsm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=1)\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train)",
    "output": "AdaBoostClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "print(\"Before OverSampling, count of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, count of label '0': {} \\n\".format(sum(y_train == 0)))\n\nprint(\"After OverSampling, count of label '1': {}\".format(sum(y_train_over == 1)))\nprint(\"After OverSampling, count of label '0': {} \\n\".format(sum(y_train_over == 0)))\n\nprint(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\nprint(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))",
    "output": "0.7453284757315783\n0.5265852697912564",
    "image_path": null
  },
  {
    "code": "# training the decision tree model with oversampled training set\ndtree2.fit(X_train_over, y_train_over)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = dtree2.predict(X_train_over)\npred_val = dtree2.predict(X_val)",
    "output": "",
    "image_path": "extracted_images_beta\\image_26.png"
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "GradientBoostingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "0.7630743918204255\n0.5246159905474597",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with oversampled training set\nbagging_estimator.fit(X_train_over, y_train_over)",
    "output": "",
    "image_path": "extracted_images_beta\\image_27.png"
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = bagging_estimator.predict(X_train_over)\npred_val = bagging_estimator.predict(X_val)",
    "output": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=1, ...)",
    "image_path": null
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "0.8419320719238453\n0.4970460811343048",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "",
    "image_path": "extracted_images_beta\\image_28.png"
  },
  {
    "code": "# training the Bagging estimator with oversampled training set\nrf_estimator.fit(X_train_over, y_train_over)",
    "output": "Before Under Sampling, count of label '1': 4231\nBefore Under Sampling, count of label '0': 8509 \n\nAfter Under Sampling, count of label '1': 4231\nAfter Under Sampling, count of label '0': 4231 \n\nAfter Under Sampling, the shape of train_X: (8462, 16)\nAfter Under Sampling, the shape of train_y: (8462,)",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = rf_estimator.predict(X_train_over)\npred_val = rf_estimator.predict(X_val)",
    "output": "DecisionTreeClassifier(max_depth=np.int64(8), max_leaf_nodes=np.int64(10),\n                       min_samples_split=np.int64(10), random_state=42)",
    "image_path": null
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "0.6908532261876625\n0.6738873572272548",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "",
    "image_path": "extracted_images_beta\\image_29.png"
  },
  {
    "code": "# training the Bagging estimator with oversampled training set\nadaboost.fit(X_train_over, y_train_over)",
    "output": "BaggingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = adaboost.predict(X_train_over)\npred_val = adaboost.predict(X_val)",
    "output": "0.9718742614039234\n0.6309570697124852",
    "image_path": null
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_30.png"
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "RandomForestClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with oversampled training set\ngboost.fit(X_train_over, y_train_over)",
    "output": "1.0\n0.6849153209925167",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = gboost.predict(X_train_over)\npred_val = gboost.predict(X_val)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "",
    "image_path": "extracted_images_beta\\image_31.png"
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "AdaBoostClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "0.6589458756795084\n0.650649862150453",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with oversampled training set\nxgboost.fit(X_train_over, y_train_over)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = xgboost.predict(X_train_over)\npred_val = xgboost.predict(X_val)",
    "output": "",
    "image_path": "extracted_images_beta\\image_32.png"
  },
  {
    "code": "# Checking recall score on oversampled train and validation set\nprint(recall_score(y_train_over, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "GradientBoostingClassifier(random_state=1)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for oversampled train data\ncm = confusion_matrix(y_train_over, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "0.6974710470337981\n0.6612839700669555",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# fit random under sampler on the train data\nrus = RandomUnderSampler(random_state=1, sampling_strategy = 1)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)",
    "output": "",
    "image_path": "extracted_images_beta\\image_33.png"
  },
  {
    "code": "print(\"Before Under Sampling, count of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before Under Sampling, count of label '0': {} \\n\".format(sum(y_train == 0)))\n\nprint(\"After Under Sampling, count of label '1': {}\".format(sum(y_train_un == 1)))\nprint(\"After Under Sampling, count of label '0': {} \\n\".format(sum(y_train_un == 0)))\n\nprint(\"After Under Sampling, the shape of train_X: {}\".format(X_train_un.shape))\nprint(\"After Under Sampling, the shape of train_y: {} \\n\".format(y_train_un.shape))",
    "output": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=1, ...)",
    "image_path": null
  },
  {
    "code": "# training the decision tree model with undersampled training set\ndtree2.fit(X_train_un, y_train_un)",
    "output": "0.8922240605057906\n0.6703426545884207",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = dtree2.predict(X_train_un)\npred_val = dtree2.predict(X_val)",
    "output": "Text(58.222222222222214, 0.5, 'Actual Values')",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersampled train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "",
    "image_path": "extracted_images_beta\\image_34.png"
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "Fitting 5 folds for each of 30 candidates, totalling 150 fits\nCPU times: user 360 ms, sys: 94.1 ms, total: 454 ms\nWall time: 5.45 s",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "np.float64(0.5065077411305652)",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with undrsampled training set\nbagging_estimator.fit(X_train_un, y_train_un)",
    "output": "DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=2,\n                       min_samples_split=5, splitter='random')",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = bagging_estimator.predict(X_train_un)\npred_val = bagging_estimator.predict(X_val)",
    "output": "Recall on train and validation set\n0.5566060033089104\n0.49940921622686096\n\nPrecision on train and validation set\n0.6960945191992124\n0.6384039900249376\n\nAccuracy on train and validation set\n0.7620094191522763\n0.7373103087388801",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersample train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "Fitting 5 folds for each of 30 candidates, totalling 150 fits",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "np.float64(0.4812147448511085)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n  warnings.warn(",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with undersampled training set\nrf_estimator.fit(X_train_un, y_train_un)",
    "output": "Recall on train and validation set",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = rf_estimator.predict(X_train_un)\npred_val = rf_estimator.predict(X_val)",
    "output": "Fitting 5 folds for each of 30 candidates, totalling 150 fits\nCPU times: user 3.55 s, sys: 598 ms, total: 4.14 s\nWall time: 5min 30s",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersampled train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "np.float64(0.4868885037163567)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "GradientBoostingClassifier(learning_rate=1.0, max_depth=5, max_features=0.1,\n                           min_samples_leaf=2, min_samples_split=20,\n                           n_estimators=50)",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "Recall on train and validation set\n0.5613330181990073\n0.47459629775502166\n\nPrecision on train and validation set\n0.7097518753606463\n0.6112214498510427\n\nAccuracy on train and validation set\n0.7783359497645211\n0.7234432234432234",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with undersampled training set\nadaboost.fit(X_train_un, y_train_un)",
    "output": "Recall on test set\n0.4781323877068558\n\nPrecision on test set\n0.6524193548387097\n\nAccuracy on test set\n0.7421507064364207",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = adaboost.predict(X_train_un)\npred_val = adaboost.predict(X_val)",
    "output": "Recall on test set\n0.516548463356974\n\nPrecision on test set\n0.649331352154532\n\nAccuracy on test set\n0.7468602825745683",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersampled train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "Recall on test set\n0.5135933806146572\n\nPrecision on test set\n0.6145685997171145\n\nAccuracy on test set\n0.7315541601255887",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "",
    "image_path": "extracted_images_beta\\image_35.png"
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with undersampled training set\ngboost.fit(X_train_un, y_train_un)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = gboost.predict(X_train_un)\npred_val = gboost.predict(X_val)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersampled train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "",
    "image_path": null
  },
  {
    "code": "# training the Bagging estimator with undersampled training set\nxgboost.fit(X_train_un, y_train_un)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Predicting the target for train and validation set\npred_train = xgboost.predict(X_train_un)\npred_val = xgboost.predict(X_val)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Checking recall score on undersampled train and validation set\nprint(recall_score(y_train_un, pred_train))\nprint(recall_score(y_val, pred_val))",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for undersampled train data\ncm = confusion_matrix(y_train_un, pred_train)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Confusion matrix for validation data\ncm = confusion_matrix(y_val, pred_val)\nplt.figure(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt=\"g\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()",
    "output": "",
    "image_path": null
  },
  {
    "code": "%%time\n\n# Choose the type of classifier.\nrf2 = DecisionTreeClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {\n    'max_depth': [3, 5, 7, 10, 15, 20, None],\n    'min_samples_split': [2, 5, 10, 20, 50],\n    'min_samples_leaf': [1, 2, 4, 8, 10],\n    'max_features': ['sqrt', 'log2', None],\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random']\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the random search\ngrid_obj = RandomizedSearchCV(rf2, parameters,n_iter=30, scoring=acc_scorer,cv=5, random_state = 1, n_jobs = -1, verbose = 2)\n# using n_iter = 30, so randomized search will try 30 different combinations of hyperparameters\n# by default, n_iter = 10\n\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Print the best combination of parameters\ngrid_obj.best_params_",
    "output": "",
    "image_path": null
  },
  {
    "code": "grid_obj.best_score_",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Set the clf to the best combination of parameters\ndt2_tuned = DecisionTreeClassifier(\n splitter= 'random',\n min_samples_split= 5,\n min_samples_leaf= 2,\n max_features= None,\n max_depth= 10,\n criterion= 'entropy'\n)\n\n# Fit the best algorithm to the data.\ndt2_tuned.fit(X_train, y_train)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Checking recall score on train and validation set\nprint(\"Recall on train and validation set\")\nprint(recall_score(y_train, dt2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(recall_score(y_val, dt2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Precision on train and validation set\")\n# Checking precision score on train and validation set\nprint(precision_score(y_train, dt2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(precision_score(y_val, dt2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Accuracy on train and validation set\")\n# Checking accuracy score on train and validation set\nprint(accuracy_score(y_train, dt2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(accuracy_score(y_val, dt2_tuned.fit(X_train, y_train)\n.predict(X_val)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "%%time\n\n# Choose the type of classifier.\nadaboost2 = AdaBoostClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {\n    'n_estimators': [50, 100, 200, 300, 400],\n    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n    'estimator': [\n        DecisionTreeClassifier(max_depth=1),  # Decision stump\n        DecisionTreeClassifier(max_depth=2),  # Slightly deeper trees\n        DecisionTreeClassifier(max_depth=3)   # Deeper trees\n    ],\n    'algorithm': ['SAMME', 'SAMME.R']\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the random search\ngrid_obj = RandomizedSearchCV(adaboost2, parameters,n_iter=30, scoring=acc_scorer,cv=5, random_state = 1, n_jobs = -1, verbose = 2)\n# using n_iter = 30, so randomized search will try 30 different combinations of hyperparameters\n# by default, n_iter = 10\n\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Print the best combination of parameters\ngrid_obj.best_params_",
    "output": "",
    "image_path": null
  },
  {
    "code": "grid_obj.best_score_",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Set the clf to the best combination of parameters\nada2_tuned = AdaBoostClassifier(\n n_estimators= 300,\n learning_rate= 1.0,\n estimator= DecisionTreeClassifier(max_depth=3),\n algorithm= 'SAMME'\n)\n\n# Fit the best algorithm to the data.\nada2_tuned.fit(X_train, y_train)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Checking recall score on train and validation set\nprint(\"Recall on train and validation set\")\nprint(recall_score(y_train, ada2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(recall_score(y_val, ada2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Precision on train and validation set\")\n# Checking precision score on train and validation set\nprint(precision_score(y_train, ada2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(precision_score(y_val, ada2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Accuracy on train and validation set\")\n# Checking accuracy score on train and validation set\nprint(accuracy_score(y_train, ada2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(accuracy_score(y_val, ada2_tuned.fit(X_train, y_train)\n.predict(X_val)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "%%time\n\n# Choose the type of classifier.\ngboost2 = GradientBoostingClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {\n    'n_estimators': [50, 100, 200, 300, 400],  # Number of boosting stages\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0],  # Learning rate\n    'max_depth': [3, 4, 5, 6, 7],  # Maximum depth of the trees\n    'min_samples_split': [2, 5, 10, 20],  # Min samples to split a node\n    'min_samples_leaf': [1, 2, 5, 10],  # Min samples at a leaf node\n    'max_features': ['sqrt', 'log2', None, 0.1, 0.3, 0.5, 1],  # Max features for splitting nodes\n    'subsample': [0.5, 0.7, 0.8, 1.0],  # Fraction of samples used for fitting each tree\n    'loss': ['log_loss', 'exponential']  # Loss function to optimize\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the random search\ngrid_obj = RandomizedSearchCV(gboost2, parameters,n_iter=30, scoring=acc_scorer,cv=5, random_state = 1, n_jobs = -1, verbose = 2)\n# using n_iter = 30, so randomized search will try 30 different combinations of hyperparameters\n# by default, n_iter = 10\n\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Print the best combination of parameters\ngrid_obj.best_params_",
    "output": "",
    "image_path": null
  },
  {
    "code": "grid_obj.best_score_",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Set the clf to the best combination of parameters\ngboost2_tuned = GradientBoostingClassifier(\n subsample= 1.0,\n n_estimators= 50,\n min_samples_split= 20,\n min_samples_leaf= 2,\n max_features= 0.1,\n max_depth= 5,\n loss= 'log_loss',\n learning_rate=1.0\n)\n\n# Fit the best algorithm to the data.\ngboost2_tuned.fit(X_train, y_train)",
    "output": "",
    "image_path": null
  },
  {
    "code": "# Checking recall score on train and validation set\nprint(\"Recall on train and validation set\")\nprint(recall_score(y_train, gboost2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(recall_score(y_val, gboost2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Precision on train and validation set\")\n# Checking precision score on train and validation set\nprint(precision_score(y_train, gboost2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(precision_score(y_val, gboost2_tuned.fit(X_train, y_train)\n.predict(X_val)))\nprint(\"\")\nprint(\"Accuracy on train and validation set\")\n# Checking accuracy score on train and validation set\nprint(accuracy_score(y_train, gboost2_tuned.fit(X_train, y_train)\n.predict(X_train)))\nprint(accuracy_score(y_val, gboost2_tuned.fit(X_train, y_train)\n.predict(X_val)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "model = dt2_tuned\n\n# Checking recall score on test set\nprint(\"Recall on test set\")\nprint(recall_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking precision score on test set\nprint(\"Precision on test set\")\nprint(precision_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking accuracy score on test set\nprint(\"Accuracy on test set\")\nprint(accuracy_score(y_test, model.predict(X_test)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "model = ada2_tuned\n\n# Checking recall score on test set\nprint(\"Recall on test set\")\nprint(recall_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking precision score on test set\nprint(\"Precision on test set\")\nprint(precision_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking accuracy score on test set\nprint(\"Accuracy on test set\")\nprint(accuracy_score(y_test, model.predict(X_test)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "model = gboost2_tuned\n\n# Checking recall score on test set\nprint(\"Recall on test set\")\nprint(recall_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking precision score on test set\nprint(\"Precision on test set\")\nprint(precision_score(y_test, model.predict(X_test)))\nprint(\"\")\n\n# Checking accuracy score on test set\nprint(\"Accuracy on test set\")\nprint(accuracy_score(y_test, model.predict(X_test)))",
    "output": "",
    "image_path": null
  },
  {
    "code": "importances = ada2_tuned.feature_importances_\nindices = np.argsort(importances)\nfeature_names = list(X.columns)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()",
    "output": "",
    "image_path": null
  },
  {
    "code": "",
    "output": "",
    "image_path": null
  }
]